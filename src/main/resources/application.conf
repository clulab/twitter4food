#
# Configuration file for the classifiers
#

# The file is compartmentalized into multiple classifiers
# and the corresponding local files needed for each

resources = "src/main/resources"

sista_resources = ${resources}/edu/arizona/sista/twitter4food

server_path = "/data/nlp/corpora/twitter4food"

captioner_home = "/work/ajaynagesh/software/chainer-caption" 


default_package = ${resources}/org/clulab/twitter4food

classifier = ${default_package}/featureclassifier
human = ${classifier}/human
overweight = ${classifier}/overweight
diabetes = ${classifier}/diabetes
gender = ${classifier}/gender
topics = ${server_path}/topics_tokenized

wordnet = ${server_path}/dict

twitter4j {
  api_keys = ${server_path}/APIKeys.txt
}

oldKeys = ${server_path}/t4fAPIKeys.txt

foursquare_key = ${server_path}/t4f4sKey.txt
places_key = ${server_path}/t4fplacesKey.txt

spam_words = ${default_package}/spam_words.txt

bigOven {
  json = ${server_path}/bigOvenJson
  img = ${server_path}/bigOvenImg
  maxIdx = 1896760
}

classifiers {
  features {
    stopWords = ${classifier}/features/stopwords.txt
    foodWords = ${sista_resources}/food_words.txt
    hashtags = ${classifier}/features/overweightHashtags.txt
    random_tweets = ${topics}/random_sample_10Oct2013.txt
    overweight_tweets = ${topics}/food_sample_2Oct2013.txt

    followerRelations = ${server_path}/followers/followerRelations.txt
    newFollowerRelations = ${server_path}/followers_tokenized/newFollowerRelations.txt
    followerAccounts = ${server_path}/followers_tokenized/followerAccounts.txt
    newFollowerAccounts = ${server_path}/followers_tokenized/newFollowerAccounts.txt

    #vectors = "/data/nlp/corpora/word2vec/gigaword/vectors.txt"
    #vectors = "/data/nlp/corpora/twitter4food/overweight_vectors_clean.txt"
    vectors = "/data/nlp/corpora/twitter4food/food_vectors_clean.txt"
  }

  diabetes {
    data = ${server_path}/diabetes/tweets_tokenized.txt
    data_raw = ${server_path}/diabetes/tweets.txt
    handles = ${server_path}/diabetes/handles.txt
    twitterImages = ${server_path}/diabetes/twitterImages
    externalImages = ${server_path}/diabetes/externalImages
    folds = ${server_path}/diabetes/folds.csv

    possibleLabels = ["risk", "not"]
  }

  overweight {
    data = ${server_path}/overweightData_tokenized/overweightData.txt
    data_verbose = ${overweight}/overweight/overweightDataVerbose.txt
    opt_template = ${overweight}/overweight/overweightData_
    data_raw = ${server_path}/newOverweightData_tokenized/merged.txt
    midrange = ${classifier}/usersMidrange.txt
    labels = ${overweight}/overweightLabels.txt
    handles = ${overweight}/overweightHandles.txt
    newHandles = ${overweight}/AnnotatedUsers.txt
    annotatedUsersFile = ${overweight}/AnnotatedUsers.txt
    annotatedFoodFile = ${overweight}/food_annotations.txt
    ageGenderAnnotations = ${server_path}/ageGenderAnnotations.csv
    twitterImageURLs = ${server_path}/twitterImageURLs
    twitterImages = ${server_path}/twitterImages
    externalImageURLs = ${server_path}/externalImageURLs
    externalImages = ${server_path}/externalImages

    heuristic_ow = ${classifier}/overweight/heuristic_ow.txt
    heuristic_no = ${classifier}/overweight/heuristic_no.txt

    location_types = ${overweight}/location_types.txt

    possibleLabels = ["Overweight", "Not overweight"]

    twFoodPerc = ${server_path}/twitterFoodPerc.tsv
    igFoodPerc = ${server_path}/instaFoodPerc.tsv

    captions = ${server_path}/captions.txt

    tweetCoords = ${server_path}/tweetCoords.txt
    tweetLocs = ${server_path}/tweetLocs.txt

    folds = ${server_path}/folds.csv
    usFolds = ${server_path}/usFolds.csv

    highConfPercent = 1.0

    "Overweight" {
      lexicons = [
        ${overweight}/food_words.txt,
        ${overweight}/restaurant_hashtags.txt,
        ${overweight}/activity_words.txt,
        ${overweight}/overweight_hashtags.txt
      ]
    }
    "Not overweight" {
      lexicons = [
        ${overweight}/food_words.txt,
        ${overweight}/restaurant_hashtags.txt,
        ${overweight}/activity_words.txt,
        ${overweight}/overweight_hashtags.txt
      ]
    }

    trainingData = ${server_path}/overweightData_tokenized/overweightTrain.txt
    devData = ${server_path}/overweightData_tokenized/overweightDev.txt
    testData = ${server_path}/overweightData_tokenized/overweightTest.txt

    stream = ${overweight}/overweight_sample_4Apr2016.txt
    opt = ${overweight}/overweightData_

    allTrainData = ${server_path}/newOverweightData/newTrainData.txt
    allTestData = ${overweight}/allTestData.txt

    results = ${overweight}/results
    baseline = "u"
    positiveLabel = "Overweight"

    humanClassifier = ${human}/model/d.dat
    genderClassifier = ${gender}/models/dtuw.dat

    followerProp = 1.0

    followeeRelations = ${server_path}/overweightData_tokenized/followeeRelations.txt
    newFolloweeRelations = ${server_path}/overweightData_tokenized/newFolloweeRelations.txt

    rawFeatures = ${server_path}/overweightData_tokenized/rawFeatures
    rawTokens = ${server_path}/overweightData_tokenized/rawTokens
  }

  human {
    annotatedUsersFile = ${human}/AnnotatedUsers.txt
    opt = ${human}/humanData_
    trainingData = ${server_path}/human_tokenized/trainingData.txt
    devData = ${server_path}/human_tokenized/devData.txt
    testData = ${server_path}/human_tokenized/testData.txt
    model = ${human}/models
    handles = ${human}/AnnotatedUsers.txt

    possibleLabels = ["human", "org"]

    human {
      lexicons = [
        ${gender}/name_all_lex.txt
        ${human}/human_lexicon.txt
      ]
    }
    org {
      lexicons = [
        ${human}/org_lexicon.txt
      ]
    }

    allTrainData = ${human}/allTrainData.txt
    predictions = ${human}/predictedLabels.txt

    results = ${human}/results
    baseline = "u"

    followeeRelations = ${server_path}/human_tokenized/followeeRelations.txt
  }

  gender {
    annotatedUsersFile = ${gender}/AnnotatedGenders.txt
    trainingFile = ${server_path}/gender_tokenized/trainingSet.txt
    devFile = ${server_path}/gender_tokenized/devSet.txt
    testFile = ${server_path}/gender_tokenized/testSet.txt
    opt = ${gender}/genderData_
    model = ${gender}/models
    handles = ${gender}/AnnotatedGenders.txt

    possibleLabels = ["F", "M"]

    M {
      lexicons = [
        ${gender}/name_both_first_lex.txt,
        //${gender}/name_both_first_last_lex.txt,
        ${gender}/name_male_first_lex.txt
        //${gender}/name_male_first_last_lex.txt
      ]
    }
    F {
      lexicons = [
        ${gender}/name_both_first_lex.txt,
        //${gender}/name_both_first_last_lex.txt,
        ${gender}/name_female_first_lex.txt
        //${gender}/name_female_first_last_lex.txt
      ]
    }

    trainingData = ${server_path}/gender_tokenized/trainingData.txt
    devData = ${server_path}/gender_tokenized/devData.txt
    testData = ${server_path}/gender_tokenized/testData.txt
    allTrainData = ${server_path}/gender_tokenized/allTrainData.txt
    predictions = ${gender}/predictedLabels.txt

    results = ${gender}/results
    baseline = "u"

    followeeRelations = ${server_path}/gender_tokenized/followeeRelations.txt
  }

  race {

  }
}


lda {
  2lineTrainingData = [
    ${classifiers.overweight.trainingData},
    ${classifiers.features.followerAccounts},
    ${classifiers.human.trainingData},
    ${classifiers.gender.trainingData}
  ]
  3lineTrainingData = [
    ${topics}/random_sample_10Oct2013.txt,
    ${topics}/food_sample_2Oct2013.txt
  ]
  stopWords = ${default_package}/lda/stopwords.txt
  modelDir = ${server_path}/lda_models
  topicModel = ${lda.modelDir}/lda_200t_1000i.model
  verbose = true
}

captioner {
	pythonPath = /work/ajaynagesh/anaconda2/bin/python,
	pythonCmd = ${captioner_home}/sample_code_beam.py,
	pythonParams = --rnn-model ${captioner_home}/data/caption_en_model40.model --cnn-model ${captioner_home}/data/ResNet50.model --vocab ${captioner_home}/data/MSCOCO/mscoco_caption_train2014_processed_dic.json --gpu -1 --img,
	outputFile = captioner.output.txt	
}
